[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started",
    "section": "",
    "text": "This page is designed to help you get a basic understanding of statistical concepts and using R before the core module content in week 10. This is self-guided content for you to go through at your own pace, with links where necessary to highlight extra resources.\nThere is a general introduction page if you are here as a complete beginner and this will cover the approach for the module and give you the tools and materials you need to get started. If you are already an RStudio user and comfortable managing and storing script files and scripts in R, you may want to jump straight into the Bootcamp modules.\nWe make a few assumptions about this material:\n\nYou follow along with the workshop pages consecutively\nYou complete each one before moving on to the next\nYou attempt all of the problems at the end of each page\n\nCrucial Reminder! Throughout these early materials you will be introduced to new terminology and processes which may at first appear daunting. Do not fear! As with any software, and programming in general, it has its peculiarities that you will soon get used to. Ultimately the aim of this resource and the EDA module as a whole is to get you comfortable with just ‘having a go’ and knowing where to look when you get stuck."
  },
  {
    "objectID": "index.html#the-script-window",
    "href": "index.html#the-script-window",
    "title": "Getting Started",
    "section": "4.1 The Script Window",
    "text": "4.1 The Script Window\nThe script window is located in the upper left of the RStudio console by default. You may need to open a script or start a new one: File &gt; New File &gt; R Script (hotkey Ctrl+Shift+N).\nThe script window is where you are likely to spend most of your time building scripts and executing commands you write. You can have many scripts open at the same time (in “tabs”), and you can have different kinds of scripts, e.g., for different parts of a project or even for programming languages."
  },
  {
    "objectID": "index.html#the-console-window",
    "href": "index.html#the-console-window",
    "title": "Getting Started",
    "section": "4.2 The Console Window",
    "text": "4.2 The Console Window\nThe Console window is in the lower left by default. Notice there are several other tabs visible, but we will only mention the Console for now. The Console is the place where text outputs will be printed (e.g. the results of statistical tests), and also is a place where R will print Warning and Error messages."
  },
  {
    "objectID": "index.html#the-global-environment",
    "href": "index.html#the-global-environment",
    "title": "Getting Started",
    "section": "4.3 The Global Environment",
    "text": "4.3 The Global Environment\nThe Global Environment is in the Environment tab in the upper right of RStudio by default. This pane is useful in displaying data objects that you have loaded and available."
  },
  {
    "objectID": "index.html#the-plots-window",
    "href": "index.html#the-plots-window",
    "title": "Getting Started",
    "section": "4.4 The Plots Window",
    "text": "4.4 The Plots Window\nThe Plots window is a tab in the lower right by default. This is the place where graphics output is displayed and where plots can be named, resized, copied and saved. There are some other important tabs here as well, which you can also explore. When a new plot is produced, the Plots tab will become active."
  },
  {
    "objectID": "index.html#script-setup",
    "href": "index.html#script-setup",
    "title": "Getting Started",
    "section": "5.1 Script Setup",
    "text": "5.1 Script Setup\nAn R script is a plain text file where the file name ends in “dot R” (.R) by default.\nAn R script serves several purposes:\nFirst, it documents your analysis allowing it to be reproduced exactly by yourself or by others.\nSecond, it is the interface between your commands and R software.\nA goal is that your scripts should contain only important R commands and information, in an organized and logical way that has meaning for other people, maybe for people you have never spoken to. A typical way to achieve this is to organize every script according to the same plan.\n\nYour R script should be a file good enough to show to a person in the future (like a supervisor, or even your future self). Someone who can help you, but also someone who you may not be able to explain the contents to. The script should be documented and complete. Think of this future person as a friend you respect.\n\nAlthough there are many ways to achieve this, for the purposes of the Bootcamp we strongly encourage you to organize you scripts like this:\n\nHeader\nContents\nOne separate section for each item of contents"
  },
  {
    "objectID": "index.html#header",
    "href": "index.html#header",
    "title": "Getting Started",
    "section": "5.2 Header",
    "text": "5.2 Header\nStart every script with a Header, that contains your name, the date of the most recent edit, and a short description of the PURPOSE of the script.\n\n# A typical script Header\n\n## HEADER ####\n## Who: &lt;your name&gt;\n## What: My first script\n## Last edited: yyyy-mm-dd (ISO 8601 date format... Google it!)\n####"
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Getting Started",
    "section": "5.3 Contents",
    "text": "5.3 Contents\nYou may want to include a contents section near the top to provide a ‘road map’ for your script & analysis. For example:\n\n# A typical script Contents section\n\n## CONTENTS ####\n## 00 Setup\n## 01 Graphs\n## 02 Analysis\n## 03 Etc"
  },
  {
    "objectID": "index.html#sec-chunk",
    "href": "index.html#sec-chunk",
    "title": "Getting Started",
    "section": "5.4 Section ‘Chunks’",
    "text": "5.4 Section ‘Chunks’\nA ‘Code chunk’ break is just a notation method used to aid the readability of the script and to provide a section for each item in your table of contents. A code chunk is just a section of code set off from other sections.\nBelow is the beginning of a typical code chunk in an R script.\n\nCode chunks must start with at least one hash sign “#”\nShould have a title descriptive of code chunk contents\nEnd with (at least) four hash signs “####”\nConsecutively numbered titles can make things very tidy\n\nFor example:\n\n## 01 This here is the first line of MY CODE CHUNK ####"
  },
  {
    "objectID": "index.html#comments",
    "href": "index.html#comments",
    "title": "Getting Started",
    "section": "5.5 Comments",
    "text": "5.5 Comments\nComments are messages that explain code in your script, and they should be used throughout every script. You can think of comments like the methods section of a scientific paper - there should be enough detail to exactly replicate and understand the script, but it should also be concise.\nComment lines begin with the # character and are not treated as “code” by R.\n\n# Make a vector of numbers &lt;--- a comment\nmy_variable &lt;- c(2,5,3,6,3,4,7)\n\n# Calculate the mean of my_variable &lt;--- another comment\nmean(my_variable)\n\n[1] 4.285714"
  },
  {
    "objectID": "index.html#running-code",
    "href": "index.html#running-code",
    "title": "Getting Started",
    "section": "5.6 Running Code",
    "text": "5.6 Running Code\nTo run your code, or ‘submit commands’, in your R script you can use a few different methods:\n\nRun the whole line of code your cursor rests on (without selecting any) Ctrl+Enter (Cmd+Return in Macs)\nRun code you have selected with your cursor Ctrl+Enter (Cmd+Return in Macs).\nUse the “Run” button along the top of the Script window\nRun code from the menu Code &gt; Run Selected Line(s)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "data-frames.html",
    "href": "data-frames.html",
    "title": "Data Frames",
    "section": "",
    "text": "NB: for this page we assume you have access to Microsoft Excel. However, similar spreadsheet software (like Libre Office Calc) will work fine.\n\nThe first step in using R for data analysis is getting your data into R. The first step for getting your data into R is making your data tidy.\n\nThe commonest question we have experienced for new users of R who want to perform analysis on their data is how to get data into R. There is good news and bad news. The good news is that it is exceedingly easy to get data into R for analysis, in almost any format. The bad news is that a step most new users find challenging is taking responsibility for their own data.\nWhat we mean here is that best practice in data management involves active engagement with your dataset. This includes choosing appropriate variable names, error checking, and documenting information about variables and data collection. We also aim to avoid proliferation of excessive dataset versions, and, worst of all, embedding graphs and data summaries into Excel spreadsheets with data.\nOn this page you will find:\n\nCommon data file types\nExcel, data setup, and the Data Dictionary\nGetting data into R\nManipulating variables in the Data Frame\nPractice exercises"
  },
  {
    "objectID": "data-frames.html#untidy-data",
    "href": "data-frames.html#untidy-data",
    "title": "Data Frames",
    "section": "4.1 Untidy Data",
    "text": "4.1 Untidy Data\nHave a look at the file 5-untidy.xlsx in Excel.\nThe aphid dataset is fairly small and it is readable by humans, but in its current form it is not usable for analysis in R or other statistical software and there are a few ambiguous aspects which we will explore and try to improve.\n\n\n\nUntidy data as seen in Excel\n\n\nThings to consider here:\n\nThe file contains embedded figures and summary tables\nThere is empty white space in the file (Row 1 and Column A)\nThe variable names violate several naming conventions (spaces, special characters)\nMissing data is coded incorrectly (Row 13 was a failed data reading, but records zeros for the actual measurements)\nConversion information accessory to the data is present (Row 3)\nThere is no Data Dictionary (i.e. explanation of the variables)\nThe Aphid and Diet treatments are “confounded” in their coding\nWhat the heck is the “RT” column (most of the values are identical)"
  },
  {
    "objectID": "data-frames.html#tidy-data",
    "href": "data-frames.html#tidy-data",
    "title": "Data Frames",
    "section": "4.2 Tidy Data",
    "text": "4.2 Tidy Data\n\n\n\nTidy data as seen in Excel\n\n\nDifferences to consider here:\n\nThe embedded figures have been removed\nThe white space rows and columns have been removed\nThe variable names have been edited but still are equally informative\nMissing data is coded correctly with “NA”\nThe conversion info has been removed and placed in the Data Dictionary\nA complete Data Dictionary on a new tab (“dictionary”) was added, explaining each variable\nThe aphid and food treatment variables were made separate\n\nAlso note the ‘dictionary’ tab in the tidy data file - seen below. Notice how there is a row for each variable with the name of the variable and an explanation for each variable.\n\n\n\nData dictionary, as seen in Excel"
  },
  {
    "objectID": "data-frames.html#csv-files",
    "href": "data-frames.html#csv-files",
    "title": "Data Frames",
    "section": "4.3 CSV Files",
    "text": "4.3 CSV Files\nOnce your data is tidy, it is very easy to read in Excel data files, or they can be exported into a text file format like CSV (comma separated values) to read straight into R or other programs.\nHave a look at the Tidy Data dataset in .csv file format.\nOpen it with a plain text editor (e.g. Notepad in Windows, or similar). You will notice that each column entry is separated from others with a comma ,, hence the name Comma Separated Values!\n\n\n\nTidy data in CSV format"
  },
  {
    "objectID": "data-frames.html#working-directory",
    "href": "data-frames.html#working-directory",
    "title": "Data Frames",
    "section": "5.1 Working Directory",
    "text": "5.1 Working Directory\nThere are several viable ways to set your working directory in R, e.g. via the Session menu:\n\n\n\nSetting the working directory via the Session menu\n\n\nHowever, the best way to do this this is to set your working directory using code with the setwd() function. Here we show a workflow for Windows, which is similar on other computer systems. We consider the step of setting a working directory essential for best practice.\nIf you are unfamiliar with how to obtain the path to your working directory, open windows explorer, navigate to the folder you wish to save your script, data files and other inputs and outputs. You can think of this folder as one that contains all related files for e.g. a data analysis project, or perhaps this introductory material!\n\n\n\nFolder view, with cursor indicating file path for an example working directory\n\n\nNote that the folder “view” is set to “Details”, and also notice that the folder options are set to “Show file extensions”. We recommend setting your own settings like this (if using Windows Explorer).\nThe pointer is indicated in the red circle, marked in the picture above. Left click the area to the right of the folder text once (where the pointer is in the picture above) and you should see something similar to the figure below, where the folder path is displayed and the text is automatically selected.\n\n\n\nExample of a selected file path\n\n\nAssuming you have opened the File Explorer in your working directory or navigated there, the selected PATH is the working directory path which you can copy (Ctrl + c in Windows). In your script, you can now use getwd() to get and print your working directory path, and setwd(), which takes a single character string of the path for your working directory for the argument dir , to set it.\n\nR file paths use the forward slash symbol “/” to separate file names. A very important step for Windows users when setting the working directory in R is to change the Windows default “” for forward slashes, or remember to change the backslashes once in R."
  },
  {
    "objectID": "data-frames.html#read-in-your-first-file",
    "href": "data-frames.html#read-in-your-first-file",
    "title": "Data Frames",
    "section": "5.2 Read in Your First File",
    "text": "5.2 Read in Your First File\nNOTE: For this section, use the ‘Tidy’ data file from Section 4.\nThen try this in your example script:\n\ngetwd() # Prints working directory in Console\n\nsetwd(\"D:/Dropbox/git-rstats-bootcamp/website/data\")\n\n# NB the quotes\n# NB the use of \"/\"\n# NB this is AN EXAMPLE directory - change the PATH to YOUR directory :)\n\ngetwd() # Check that change worked\n\n## Read in Excel data file\n\n\ninstall.packages(openxlsx, dep = T) # Run if needed\n\nlibrary(openxlsx) # Load package needed to read Excel files\n\n# Make sure the data file \"5-tidy.xlsx\" is in your working directory\nmy_data <- read.xlsx(\"5-tidy.xlsx\")\n\nAll being well, you should see the following data object in your Global Environement. Note the small blue button (circled below in red) you can press to espand the view of the variables in your data frame.\n\n\n\nData summary in your Global Environment\n\n\nNote that the same procedure works with Comma Separated Values data files, and other kinds of files that you want to read into R, except that the R function used will be specific to the file type. E.g., read.csv() for CSV files, read.delim for TAB delimited files, or read.table() as a generic function to tailor to many types of plain text data files (there are many others, but this is enough for now)."
  },
  {
    "objectID": "data-objects.html",
    "href": "data-objects.html",
    "title": "Data Objects",
    "section": "",
    "text": "The fundamental way to analyse data in R is to be able to manipulate it with code. In order to do that, we need a system of containers to store the data in. This page is about the rules used for storing data in data objects. We will examine basic data types at the same time as how R stores them. R is actually very flexible in the way it handles data, making it as easy as possible for non-programmers to get going.\nOn this page you will find:\n\nBasic data types in R, str()\nData with factors\nclass() and converting variables\nVector and Matrix fun\nPractice exercises"
  },
  {
    "objectID": "data-objects.html#the-global-environment",
    "href": "data-objects.html#the-global-environment",
    "title": "Data Objects",
    "section": "2.1 The Global Environment",
    "text": "2.1 The Global Environment\nOne of the things we notice when people begin using R, even if they are experienced in data analysis, is that they expect to “see” data and data sets, almost as they are physical things. This might be because of experience using Excel and seeing the visual representation of data in spreadsheets (indeed, a graphical representation of physical spreadsheets!).\nThe regular R system for interacting with data is a little more abstract which can be disconcerting to new users. Typical use is to create variables in code script files and, usually, the bring data into the Global Environment from external files on the local PC, or from the web. We will practice using the Global Environment is the main way to interact with data.\nYou can use the class() function to find out the variable type (using this this is a good idea, since R occasionally guesses the intended data type incorrectly).\nTry this in your example script:\n\nvariable_1 <- c(4,5,7,6,5,4,5,6,7,10,3,4,5,6) # a numeric vector\n\nvariable_2 <- c(TRUE, TRUE, TRUE, FALSE) # a logical vector\n\nvariable_3 <- c(\"Peter Parker\", \"Bruce Wayne\", \"Asterix the Gaul\") # a character vector\n\nclass(variable_1) # \"numeric\"\nclass(variable_2) # \"logical\"\nclass(variable_3) # \"character\"\n\nLook at the upper right pane of your RStudio window and you should see something like this:\n\n\n\nRStudio Global Environment\n\n\nThe Environment tab contains the Global Environment (labelled A in the picture above). There are some other tabs, but we will ignore these for now. The Global Environment itself contains information about the variables that are held in memory. If we think of this as “R Space”, a general rule is that if you can see a variable here in the Global Environment, you can manipulate it and work with it.\nNotice that there is quite a lot of information in the Global Environment about the actual variables (B in the picture). There is a column with the variable NAME (variable_1, variable_2, etc.), a column with the variable TYPES (num, logi, etc.), a column with the variable dimensions ([1:14] is an index like a unique “street address” for each of the 14 numeric values contained in “variable_1”)"
  },
  {
    "objectID": "data-objects.html#naming-conventions-for-variables",
    "href": "data-objects.html#naming-conventions-for-variables",
    "title": "Data Objects",
    "section": "2.2 Naming conventions for variables",
    "text": "2.2 Naming conventions for variables\nVariable names:\n\nCan contain letters, numbers, some symbolic characters\nMust begin with a letter\nMust not contain spaces\nSome forbidden characters like math operators, “@”, and a few others\nShould be human-readable, consistent, and not too long\nCase sensitive\n\nTry this in your example script:\n\n## Variable name rules ####\n\n# Can contain letters, numbers, some symbolic characters\nx1 <- 5  # OK\n\nx2 <- \"It was a dark and stormy night\" # OK\n\nmy_variable_9283467 <- 1 # technically works, but hard to read\n\n# Must begin with a letter \n\nvarieties <- c(\"red delicious\", \"granny smith\") # OK\n\nx432 <- c(\"a\", \"b\") # OK\n\n22catch <- c(TRUE, TRUE, FALSE)  # nope\n\n# Must not contain spaces\nmy_variable <- 3 # OK\n\nmy.variable <- 4 # OK\n\nmyVariable <- 5 # OK\n\nmy variable <- 6 # nope\n\n\"my variable\" <- 7 # nope\n\n# Must not contain forbidden characters like \n# math operators, \"@\", and a few others\nmy@var <- 1 # nope\n\nmy-var <- 1 # nope\n\nmy=var <- 1 # nope\n\n# etc.\n\n# Should be human-readable, consistent, and not too long\n\nDiameter_Breast_Height_cm <- c(22, 24, 29, 55, 43) # legal but too long\n\nDBH_cm <- c(22, 24, 29, 55, 43) # much better\n\n#Case sensitive\nheight <- c(180, 164, 177) # OK\n\nHeight # Error: object 'Height' not found (notice capital H)\n\nheight # OK"
  },
  {
    "objectID": "data-objects.html#vectors",
    "href": "data-objects.html#vectors",
    "title": "Data Objects",
    "section": "5.1 Vectors",
    "text": "5.1 Vectors\nTry this in your example script:\n\nmyvec1 <- c(1,2,3,4,5) # numeric vector\nmyvec1\nclass(myvec1) # see? I told ya!\n\nmyvec2 <- as.character(myvec1) #convert to character\nmyvec2 # notice the quotes\nclass(myvec2) # now character\n\nmyvec3 <- c(2, 3, \"male\")\nmyvec3 #notice the numbers now have quotes - forced to character...\n\nmyvec4 <- as.numeric(myvec3) #notice the warning\nmyvec4 # The vector element that could not be coerced to be a numeric was converted to NA"
  },
  {
    "objectID": "data-objects.html#matrices",
    "href": "data-objects.html#matrices",
    "title": "Data Objects",
    "section": "5.2 Matrices",
    "text": "5.2 Matrices\nMatrices can be quite useful - you can manipulate data into matrix form with the matrix() function. By default rows and columns are merely numbered, but they can be named as well.\nTry this in your example script:\n\nvec1 <- 1:16 # make a numeric vector with 16 elements\nvec1 \n\nhelp(matrix) #notice the ncol, nrow and byrow arguments\n\nmat1 <- matrix(data = vec1, ncol = 4, byrow = FALSE) #byrow = FALSE is the default\n\nmat1 # Notice the numbers filled in by columns\ncolnames(mat1) # The Columns and Rows have no names\n\ncolnames(mat1) <- c(\"A\", \"B\", \"C\", \"D\") # Set the column names for mat1\ncolnames(mat1)\n\nmat1 # Yep the columns shows names\n\nAnd for an extra challenge:\n\n# Challenge 1: Set the Row names for Mat1 using the rownames() function\n\n# Challenge 2: make a matrix with 3 rows with the following vector, \n# so the the first COLUMN contains the numbers 2, 5, and 9, in the order,\n# for rows 1, 2, and 3 respectively:\n\nvec2 <- c(2,3,5,4,5,6,7,8,9,5,3,1)"
  },
  {
    "objectID": "functions-and-packages.html",
    "href": "functions-and-packages.html",
    "title": "Functions and Packages",
    "section": "",
    "text": "One of the best things about R is that it can be customized to accomplish a huge variety of kinds of tasks: perform all sorts of statistical analyses from simple to bleeding edge, produce professional graphs, format analyses into presentations, manuscripts and web pages, collaboration, GIS and mapping, and a lot more. The tools themselves are contained in toolboxes and in a given toolbox, the tools are related to each other, usually focused for the kind of tasks they are suited for.\n\nThe tools in R are functions, and the toolboxes are packages.\n\nWhile R comes with a lot of packages, there is an enormous amount more available for instant download at any time you want or need them (over 18,000 different packages at the moment…). Making use of all these resources is usually a case of identifying a problem to solve, finding the package that can help you, and then learning how to use the functions in the package This page is all about introducing functions, packages and the R help system.\nOn this page you will find:\n\nFunction tour\nUsing functions and getting help\nR packages\nFinding, downloading and using packages\nPractice exercises"
  },
  {
    "objectID": "functions-and-packages.html#using-functions",
    "href": "functions-and-packages.html#using-functions",
    "title": "Functions and Packages",
    "section": "2.1 Using functions",
    "text": "2.1 Using functions\nFunctions are typically used by providing some information inside the brackets, usually data for the function to do work on or settings for the function. Function values and settings are assigned to function arguments and most functions have several arguments.\nfunction_name(argument_1 = value_1, argument_2 = value_2, ...)\nA general rule is that you INPUT information or data into function brackets that you want the function to do work and function OUTPUT is the work being done, sometimes including information output (like the results of a statistical test, or a plot).\n\nEach argument has a unique name\nArgument values are assigned using the equals sign =, the assignment operator\nEach argument is separated by a comma ,\nThe ... means there are additional arguments that can be used optionally (for now we can ignore those)"
  },
  {
    "objectID": "functions-and-packages.html#function-names",
    "href": "functions-and-packages.html#function-names",
    "title": "Functions and Packages",
    "section": "2.2 Function Names",
    "text": "2.2 Function Names\nFinding functions by their names is often easy for very simple and common tasks. For example:\nmean() Calculates the arithmetic mean\nlog() Calculates the log\nsd() Calculates the standard deviation\nplot() Draws plots\nboxplot() Draws boxplots\nhelp() Used to access help pages\nYou get the idea…\nThe most important thing here is that you would generally get the help page up as a reference to what arguments are required and how to customize your function use. This is the key to learning R in the easiest way. That is, until you memorize the use and arguments for common functions.\nREMEMBER! You are not expected to be bilingual in programming language (unless you want to be!). Just increase your familiarity and you’ll develop confidence in looking in the right places for help."
  },
  {
    "objectID": "functions-and-packages.html#finding-downloading-and-using-packages",
    "href": "functions-and-packages.html#finding-downloading-and-using-packages",
    "title": "Functions and Packages",
    "section": "4.1 Finding, downloading and using packages",
    "text": "4.1 Finding, downloading and using packages\nFinding packages happens a variety of ways in practice. A package may be recommended to you, you might be told to use a particular package for a task or assignment, or you may discover it on the web.\nInstalling and loading packages with code:\nThere are 2 steps here - installing, then loading. Installing is very easy to do using the install.packages() function. Loading a package making the functions in it available for use is done using the library() package. Basic usage of these functions is:\n\n# Step 1: install a package\n\nhelp(install.packages) # just have a look\ninstall.packages(pkgs = \"package_name\")\n\n# The package is downloaded from a remote repository, often\n# with additional packages that are required for use.\n\n# Step 2: load a package\n\nlibrary(\"package_name\")\n\n# Challenge: Install and load the \"ggplot2\" package, and then use help() to look at the help page for the function ggplot().  What kind of R object is required by the \"data\" argument?"
  },
  {
    "objectID": "functions-and-packages.html#installing-and-loading-packages-with-the-rstudio-packages-tab",
    "href": "functions-and-packages.html#installing-and-loading-packages-with-the-rstudio-packages-tab",
    "title": "Functions and Packages",
    "section": "4.2 Installing and loading packages with the RStudio Packages tab",
    "text": "4.2 Installing and loading packages with the RStudio Packages tab\nYou can find the packages tab in RStudio in the lower left pane by default.\n\n\n\nPackage tab and its elements\n\n\nWhen you click on the Packages tab (A in the picture above), you can see a list of packages that are available to you (i.e., in RStudio desktop these have already been downloaded locally).\nIn order to load a package, you can find the package name in the list and click the radio button (B in the picture).\nTo install a package, you can click on the Install button (C in the image).\nYou should see the Install Packages window, where you can enter the name of a package for installation, searching the official Comprehensive R Archive Network (Repository (CRAN)) by default:\n\n\n\nInstall packages window in RStudio"
  },
  {
    "objectID": "question-explore-analyse.html",
    "href": "question-explore-analyse.html",
    "title": "Question, Explore and Analyse",
    "section": "",
    "text": "1. Overview\nThe very first task for any data analysis is to gain an understanding of the data itself. This typically involves examining the variables (Are they as we expect? Do we need to adjust the variable types?), graphing the data, and possibly examining numerical summaries and statistical assumptions. Further, it is necessary to look for errors in the data both trivial (e.g. misspelling factor level names like “control” with an extra space “control”), and more serious such as numerical typographical errors (e.g. misplacing a decimal point is a classic: height of 5 men in feet: c(5.5, 5.8, 6.1, 5.9, 52)… ). In total, this part of data analysis is sometimes referred to as Exploratory Data Analysis.\nExploratory Data Analysis (EDA) is part practical and part philosophical in that is requires skill and experience, but is also subjective. Think of it as a step that might take a long while, where the data scientists decides what the analysis is that will be applied to the data, that the analysis is correct and appropriate. Ironically, while EDA is considered very important and can take a large proportion of the total time spent analysing data, it is usually only reported on very briefly if at all.\nThe order of operation for most analyses should be 1 question, 2 explore, 3 analyse. Focus on the question and make sure it is clear in formulation, and choose an analysis approach that can resolve the question (given the data… but the data collection should be designed to fit the question and chosen analysis prior to collection). Explore the data to examine any assumptions required for the analysis, including the use of graphing and any diagnostic or summary statistics. Finally, perform and summarize the analysis. We will practice this workflow for different basic questions in this module, with an emphasis on simple quantitative data.\nOn this page you will find:\n\nQuestion Formulation & Hypothesis Testing\nSummarise\nVariables & Graphing\nAnalysis vs ‘EDA’\nExercises\n\n\n\n2. Question Formulation & Hypothesis Testing\n\nIt is the primary responsibility of the data collector and the data scientist to agree on the specific details of generating evidence from data (i.e., statistical analysis) to answer questions. When these roles are occupied by the same person, this matter should be settled before collecting any data.\n\nThe general topic of formulating statistical questions is vast; many books have been written on the subject. The tradition and practice of statistical analysis has evolved through time. Here we will focus on the traditional starting point for a “first statistics course”, within the context of Null Hypothesis Significance testing (NHST).\nSampling concept and NHST\nThe gambit of NHST is that there is a population of interest but that the population cannot be directly measured because it is too big or otherwise inconvenient or impossible to measure. Thus, experimental samples are drawn randomly from the population, possibly subjected to experimental conditions, and the magnitude of observed differences or measured associations are summarized by various test statistics and compared to how likely such an observed difference or association would be to observe in the absence of the hypothesized effect (the latter is referred to as the null hypothesis). Finally, a P-value, the (conditional) probability of the observed effect size relative to the null hypothesis of no difference, is used to decide if the observed difference is “signficant”.\nTraditionally, the P-value is compared to the alpha value, almost always set to 0.05. This alpha value can be interpreted as the maximum probability that is acceptable of making a mistake and concluding there IS a difference, when in fact a difference does not exist. When the P-value is less than 0.05, we conclude there is a difference, rejecting the null hypothesis and “accepting” the hypothesis we predicted was true (usually referred to as the alternative hypothesis)\nNHST notes\nThe NHST is sometimes viewed contemporarily as a little deficient in practice. The reasons for this are complicated, but relate to alternative statistical frameworks that are now possible due to the development of statistical theory (e.g. Bayesian statistics, the Generalized Linear Model) and computational methods (e.g. machine learning). Nevertheless, it is still in standard use in many fields and remains the staple for the “first statistics class”.\n\n\n\n\n\n\n\nBenefits of NHST\nCriticisms of NHST\n\n\n\n\nFamiliar and acceptable to majority of researchers\nOften conceived, applied and interpreted under error\n\n\nTypically robust to assumptions when applied correctly\nValidation of analysis (e.g. assumptions testing) is often neglected\n\n\nStrong framework for evidence, especially for experiments\nEducation for applied researchers deficient in sciences\n\n\nThe basic idea is objective and simple\nThough simple, practitioners may be ignorant of subtle concepts\n\n\n\nFurther reading: If the idea is new to you that NHST in statistics is not perfect and you want to get serious about understanding why, like most subjects, you will need to pursue further sources.\nAnderson, D.R., Burnham, K.P. and Thompson, W.L., 2000. Null hypothesis testing: problems, prevalence, and an alternative. The journal of wildlife management, pp.912-923.\nNickerson, R.S., 2000. Null hypothesis significance testing: a review of an old and continuing controversy. Psychological methods, 5(2), p.241.\nNix, T.W. and Barnette, J.J., 1998. The data analysis dilemma: Ban or abandon. A review of null hypothesis significance testing. Research in the Schools, 5(2), pp.3-14.\nStephens, P.A., Buskirk, S.W., Hayward, G.D. and Martinez Del Rio, C., 2005. Information theory and hypothesis testing: a call for pluralism. Journal of applied ecology, 42(1), pp.4-12.\n\n\n3. Summarise\n\nThe best way gain skill in handling data is to practice.\n\nIt is useful to create a summary-at-a-glance of a data set. Usually this includes graphics and statistical summary, as well a description of how much data we have. A key consideration is, also, the specification of the variables.\nWe will practice data handling with the data file chickwts.xlsx.\nDownload the file, read it into a data object in R called “chicks”, and convert the “feed” variable to a factor if necessary.\nTry this in your example script:\n\n# Download the 2.1-chickwts.xlsx file, read it into a data \n# object in R called \"chicks\", \n# and convert the \"feed\" variable to a factor if necessary.\n\n# Do not neglect looking inside the \"raw\" data file\n# Is it as you expect?  Is the data dictionary present and clear?\n\n# Load necessary libraries\nlibrary(openxlsx)\n\n# Read file\nsetwd(\"D:/Dropbox/git/DSgarage/public/data\") # NB change to YOUR file path...\nchicks &lt;- read.xlsx(\"2.1-chickwts.xlsx\")\n\n# Convert feed to factor if needed\nclass(chicks$feed) # Character\nchicks$feed &lt;- factor(chicks$feed)\nclass(chicks$feed) # Factor\n\n\nThe hypothesis voices “how you think the world works” or what you predict to be true”\n\nThe basic hypothesis we believe is true for the chicks data set might be phrased in different ways.\n\nChick weight differs after 6 weeks according to feed additive type\nMean chick weight varies according to feed additive type\nThe variance between chick weight for different feed additives is bigger than the variance within chick weight as a whole\n\nThe minimum amount of information we are usually interested in when sizing up a data set is How much data is there?, What is the central tendency (e.g. the mean, variance, etc.)?, and possibly Are there rare values?. We would typically start graphing the data right away. If we have a notion of what our questions or hypotheses are, they should inform the initial peek at the data. For example, in the chickwts data, we know our question will be related not to the overall central tendency of chick weight, but to chick weight for each individual feed type.\nWe do not approach this sizing up of the data in a workhorse fashion, merely to check a tick box. We are looking quickly for details in the data that give us insight into what the data is like. For example, we peek at whether the mean and median are close to each other (indicator our data may be Gaussian), we compare the standard deviation, variance or standard error of a numeric variable relative to different levels of a factor, to see if they are similar.\nTry this in your example script:\n\n# Summarize the whole data set\n# summary() provides summary statistics for numeric variables and counts\nsummary(chicks)\n\n# we might want to look at summary for different levels of feed\n?summary\nsummary(object = chicks$weight[which(chicks$feed == \"casein\")])\nsummary(object = chicks$weight[which(chicks$feed == \"horsebean\")])\n# etc. - this method is easy but inelegant?\n\n# aggregate()\n?aggregate\n\n# mean\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = mean)\n\n# standard deviation\naggregate(x = chicks$weight, by = list(chicks$feed), FUN = sd)\n\n# You can make your own function for the FUN argument\n# stadard error of mean, SEM = standard deviation / square root of sample size\naggregate(x = chicks$weight, by = list(chicks$feed), \n          FUN = function(x){ sd(x)/sqrt(length(x)) })\n\n# You can apply several functions and name them!\naggregate(x = chicks$weight, by = list(feed = chicks$feed), \n          FUN = function(x){ c(mean = mean(x), \n                               sd = sd(x),  \n                               SEM = sd(x)/sqrt(length(x)))})\n\n\n\n4. Variables & Graphing\nA good graph usually tells the whole story, but a bad graph is worse than no graph at all.\n\n\n\nxkcd.com\n\n\nThere are a few topics in graphing data that are important to consider here, but the topic is wide and deep, analytical, creative and even artistic. We make a distinction between graphs used to explore data during EDA (meant to be “consumed” only by the data scientist who made them and are of no use to document a pattern to others) and graphs indended to document information. For both kinds of graphs, best practice is develop best practice principles. \nScientific graphs:\n\nMust convey the relevant information\nShould be consistent in aesthetics\nMust be self-contained (meaning is contained 100% within the figure and legend)\nShould reflect a hypothesis or statistical concept (if not purely descriptive)\nShould be appropriate to the data\n\nYou can think of R graphics as a way to “build up information in layers” onto a graph. There are many aesthetic features of graph that can be controlled, like adding colors, text, lines, legends, etc. The R graphics system is very simple to use, but can also be very powerful (mastering this takes practice). We make a distinction here between R base graphics and packages that can be used to make specialized and varied graphs (like the powerful and popular package {ggplot})\nLayering information\nWe can look at graphing the chicks data in a few different ways. We will try a few different graphs in this way, building up features. We might build up features on a graph using arguments in a particular graph function (like the main title with the argument main, or the x axis title with the argument xlab), or by adding features with additional function to “layer” onto the base graph (like adding lines with the functions abline() or lines()).\nTypically you would choose the type of graph that both fits the type of data you have and that conveys the information you wish to examine or showcase. E.g., for a single numeric variable, you might wish to show:\n\nThe distribution of data with a histogram: hist()\nThe central tendency relative to a factor with a boxplot: boxplot()\n\nTry the following in your example script:\n\nHistogram of the chicks data\n\n## help(hist) - if needed\nhist(x = chicks$weight)\n\nAdd a title with main\n\n# Argument main\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\")\n\nAdd an x axis title with xlab\n\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\nAdd a vertical line for the weight mean with abline()\n\nhist(x = chicks$weight,\n     main = \"Distribution of chick weights (all feeds)\",\n     xlab = \"Chick weight (grams)\")\n\n# help(abline) - if needed\nabline(v = mean(chicks$weight), col = \"red\", lty = 2, lwd = 3)\n\nTry a boxplot()\n\n# help(boxplot) - if needed\nboxplot(x = chicks$weight)\n# I have seen worse graphs, but I can't remember when.\n# Flash challenge: Fix. It.\n\nWeight as a function of feed\n\nboxplot(formula = weight ~ feed,\n        data = chicks)\n# This is probably a good representation of our hypothesis\n# Flash challenge: Fix the graph...\n\n\n\n\n5. ‘Analysis’ vs. ‘EDA’\nAlthough you could consider Exploratory Data Analysis, EDA, as an important part of the complete process of data analysis, we might make a distinction between “Analysis” the part of analysis that generates Evidence, and that of EDA that is used to explore data and test assumptions.\n\n\n\n\n\n\n\nAnalysis\nEDA\n\n\nDesigned to fit a specific question or hypothesis\nInformal and may be haphazard\n\n\nPart of a workflow: Informal hypothesis statement (in plain language) &gt; Statistical hypothesis (specifies a or implies a statistical test) &gt; Evidence (the specific results)\nDesigned to explore or gain understanding of data\n\n\nDesigned and usually formatted to present to others, such as in a report or a scientific manuscript\nAssumptions testing\n\n\nContains only bare essentials as relates to the initial hypothesis (e.g. a good graph, the summary of a statistical analysis)\nUsually not designed to document or show to others\n\n\nShould strictly be reproducible via a script and archived data\nOccurs primarily before (every) analysis\n\n\nDone only after EDA\nMay or may not be documented to be reproducible\n\n\n\nDone before the final, evidence-generating Analysis\n\n\n\nWe can keep this concept of EDA versus Analysis in our mind while we discuss the Statistical Analysis Plan.\n\n\n6. Statistical Analysis Plan\nA Statistical Analysis Plan (SAP) is a formal document that should be used to design data analysis. One of the most important functions of the SAP is to make a formal connection between the hypothesis, the data collected and and the method of analysis that will be used to generate evidence to support or refute the hypothesis. The components of a basic SAP are:\n\nThe hypotheses stated in plain language\nEach hypothesis translated into a specific statistical model\nSpecification of data and and data collection methods\nSpecification of effect size\nJustification of sample size through power analysis or other means\n\nDefinition of all of these components is beyond the boundaries of this Bootcamp, however the explicit connection of hypotheses with a statistical model is one of the very basic elements of best practice in science.\nThe Scientific Method\nWe usually learn the scientific method as a cycle where we conceive a problem, form a hypothesis, conduct an experiment, evaluate the result and so on. We teach this as a literal cycle.\n\n\n\nThe Classic Scientific Method\n\n\nThis classic view of the scientific process implies that we plan the analysis only after we conduct the experiment and collect data. While many data scientists or statisticians would agree that this model is widely used in science, it is considered very poor practice for several reasons.\n\nThe expected difference or relationship (i.e., the effect size) should explicitly be part of the hypothesis and quantified BEFORE collecting data\nThe statistical test must be chosen prior to collect the data to insure the evidence matches the expectation\nThe sample size should be justified, using power analysis or a less formal means. Collecting too little data will likely result in failing to detect a difference (if if your hypothesis is correct!); Collecting too much data is simply a waste of resources.\n\n\n\n\nThe Classic Scientific Method - does this suggest we only think about analysis AFTER data?\n\n\nBest practice scientific process\nThe traditional view of the scientific method should probably be adjusted to explicitly accomodate planning the analysis at the same time as the hypothesis formulation stage. Likewise, the analysis plan should specifically influence the design of the data collection for the experiment.\n\n\n\nThe Modern Scientific Method\n\n\nA modern view of best practice of scientific endeavor incudes an experimental design phase, that includes consideration of effect size and power analysis, and the production of a statistical analysis plan that contains a formal statistical hypothesis. All off this happens prior to any data collection.\n\n\n7. Exercises\nFor the following questions, use the field-trial.xlsxdata set. This is real data in Tidy Data format, but our information for these exercises is limited precisely to the contents of the file, including the data dictionary. In this experiment, seeds were raised under field trial conditions for 2 weeks to look at the effect of different treatment conditions on mass gain during germination. There are several measured variables, with the calculated pct variable probably intended to be the dependent variable, with the factor treatment being the main explanatory variable for variation in pct.\n\nShow code to set up an R analysis file with a header, table of contents, and a setup section that sets your working directory, loads any required libraries and reads in the data. Call the data.frame object you create seed.\npct, wet and dry should be numeric; block and trial should be factors, and treatment should be a factor with the level “Control” set as the reference. Show the code to do this.\nUse aggregate() to calculate the mean, standard deviation, standard error, and the count (e.g. length()) of pct for each level of treatment. Show the code.\nMake a fully labelled boxplot of the pct as a function of treatment. Add a horizontal line (red and dashed) for the overall mean of pct, and 2 horizontal lines (gray, dotted) for the overall mean of pct +/- 1 standard deviation.\n(hard: may require tinkering and problem solving) Experiment making a boxplot showing pct ~ treatment separated for each trial\nWrite a plausible practice question involving aggregate() and boxplot() in-built R data set iris."
  },
  {
    "objectID": "sampling-and-distribution.html",
    "href": "sampling-and-distribution.html",
    "title": "Sampling and Distribution",
    "section": "",
    "text": "I. A curve has been found representing the frequency distribution of standard deviations of samples drawn from a normal population.\nII. A curve has been found representing the frequency distribution of values of the means of such samples, when these values are measured from the mean of the population in terms of the standard deviation of the sample…\n-Gosset. 1908, Biometrika 6:25.\n\nThe idea of sampling underpins traditional statistics and is fundamental to the practice of statistics. The basic idea is usually that there is a population of interest, which we cannot directly measure. We sample the population in order to estimate the real measures of the population. Because we merely take samples, there is error assiociated with our estimates and the error depends on both the real variation in the population, but also on chance to do with which subjects are actually in our sample, as well as the size of our sample. Traditional statistical inference within Null Hypothesis Significance Testing (NHST) exploits our estimates of error associated with our samples. While this is an important concept, it is beyond the scope of this page to review it, but you may wish to refresh your knowledge by consulting a reference, such as Irizarry 2020 Chs 13-16.\nIn this page, we will briefly look at some diagnostic tools in R for examining the distribution of data, and talk about a few important distributions that are common to encounter.\nOn this page you will find:\n\nHistograms\nGaussian Distribution\nPoisson Distribution\nBinomial Distribution\nDiagnosing the distribution\nExercises"
  },
  {
    "objectID": "sampling-and-distribution.html#gaussian-histograms",
    "href": "sampling-and-distribution.html#gaussian-histograms",
    "title": "Sampling and Distribution",
    "section": "3.1 Gaussian Histograms",
    "text": "3.1 Gaussian Histograms\nWe can describe the expected perfect (i.e., theoretical) Gaussian distribution based just on the mean and variance. The value of this mean and variance control the shape of the distribution.\nGaussian Parameters = \\(N(\\overline{x}, S^2)\\)\nSample mean = \\(\\overline{x}\\) = \\(\\frac{x_{1}+x_{2},+…x_{n}}{n}\\)\nSample variance = \\(S^2\\) = \\(\\frac{\\sum(x_{i}-\\overline{x}^2}{n-1}\\) = \\((std.dev.)^2\\)\nSample size = \\(n\\)\nTry this in your example script:\n\n# 4 means\n(meanvec &lt;- c(10, 7, 10, 10))\n\n[1] 10  7 10 10\n\n# 4 standard deviations\n(sdvec &lt;- c(2, 2, 1, 3))\n\n[1] 2 2 1 3\n\n# Make a baseline plot\nx &lt;- seq(0,20, by = .1)\n\n# Probabilities for our first mean and sd\ny1 &lt;- dnorm(x = x, \n            mean = meanvec[1],\n            sd = sdvec[1])\n\n# Baseline plot of 1st mean and sd\nplot(x = x, y = y1, ylim = c(0, .4),\n     col = \"goldenrod\",\n     lwd = 2, type = \"l\",\n     main = \"Gaussian fun \n     \\n mean -&gt; curve position; sd -&gt; shape\",\n     ylab = \"Density\",\n     xlab = \"(Arbitrary) Measure\")\n\n# Make distribution lines\nmycol &lt;- c(\"red\", \"blue\", \"green\")\nfor(i in 1:3){\n  y &lt;- dnorm(x = x, \n                mean = meanvec[i+1],\n                sd = sdvec[i+1])\n  lines(x = x, y = y, \n        col = mycol[i],\n        lwd = 2, type = \"l\")\n}\n\n# Add a legend\nlegend(title = \"mean (sd)\",\n       legend = c(\"10 (2)\", \"  7 (2)\", \n                  \"10 (1)\", \"10 (3)\"),\n       lty = c(1,1,1,1), lwd = 2,\n       col = c(\"goldenrod\", \"red\", \"blue\", \"green\"),\n       x = 15, y = .35)"
  },
  {
    "objectID": "sampling-and-distribution.html#q-q-plots",
    "href": "sampling-and-distribution.html#q-q-plots",
    "title": "Sampling and Distribution",
    "section": "3.2 Q-Q Plots",
    "text": "3.2 Q-Q Plots\nIt is very often that you might want a peek or even test whether data are Gaussian. This might be in a situation when looking at, for example, the residuals for a linear model to test whether they adhere to the assumption of a Gaussian distribution. In that case, a common diagnostic graph to construct is the quantile-quantile, or “q-q”” Gaussian plot.\nThe q-q Gaussian plot your data again the theoretical expectation of the “quantile”, or percentile, were your data perfectly Gaussian (a straight, diagonal line). Remember, samples are not necessarily expected to perfectly conform to Gaussian (due to sampling error), even if the population from which the sample was taken were to be perfectly Gaussian. Thus, this is a way to confront your data with a model, to help be completely informed. The degree to which your data deviates from the line (especially systematic deviation at the ends of the line of expectation), is the degree to which is deviates from Gaussian.\nTry this in your example script:\n\nlibrary(car) # Might need to install {car}\n\nWarning: package 'car' was built under R version 4.3.2\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.3.2\n\n# Set graph output to 2 x 2 grid\n# (we will set it back to 1 x 1 later)\npar(mfrow = c(2,2)) \n\n# Small Gaussian sample\nset.seed(42)\nsm.samp &lt;- rnorm(n = 10, \n                 mean = 10, sd = 2)\n\nqqPlot(x = sm.samp, \n       dist = \"norm\", # C'mon guys, Gaussian ain't normal!\n       main = \"Small sample Gaussian\")\n\n[1] 9 2\n\n# Large Gaussian sample\nset.seed(42)\nlg.samp &lt;- rnorm(n = 1000, \n                 mean = 10, sd = 2)\n\nqqPlot(x = lg.samp, \n       dist = \"norm\", \n       main = \"Large sample Gaussian\")\n\n[1] 988 980\n\n# Non- Gaussian sample\nset.seed(42)\nuni &lt;- runif(n = 50, \n                 min = 3, max = 17)\n\nqqPlot(x = uni, \n       dist = \"norm\", \n       main = \"Big deviation at top\")\n\n[1] 35 37\n\npar(mfrow = c(1,1))"
  },
  {
    "objectID": "subsetting-and-manipulation.html",
    "href": "subsetting-and-manipulation.html",
    "title": "Subsetting and Manipulation",
    "section": "",
    "text": "Subsetting and manipulating data is probably the commonest activity for anyone who works with data. This is a core activity for exploratory data analysis, but is also extensively used in simple data acquisition, analysis and graphing, while also being related to more general data manipulating activities, for example database queries. This page is an introduction to the core syntax and some of the tools for manipulating and subsetting data in R.\nOn this page you will find:\n\nIndexing concept\nUsing which() and subsetting\nSelection on data.frame objects\nUsing aggregate()\nPractice exercises"
  },
  {
    "objectID": "subsetting-and-manipulation.html#vectors",
    "href": "subsetting-and-manipulation.html#vectors",
    "title": "Subsetting and Manipulation",
    "section": "2.1 Vectors",
    "text": "2.1 Vectors\nYou can create vector subsets by manipulating the index. Vector objects have indices in 1 dimension. For example, my_vector[1:i], where i is the length of the vector.\n\nTry this in your example script:\n\nmy_vector <- c(11.3, 11.2, 10.4, 10.4, \n               8.7, 10.8, 10.5, 10.3, 9.7, 11.2)\n\n# Return all values\nmy_vector        # Typical way\nmy_vector[ ]     # Blank index implies all index values\nmy_vector[ 1:10] # Returns all index values explicitly\n\n# Return the first 3 values\n1:3 # Reminder of the function of the colon operator \":\"\nmy_vector[ 1:3] # Notice consecutive indices can use the \":\" operator\n\n# Return 5th and 9th values\nmy_vector[ c(5, 9)] # Notice we have to place non-consecutive index values in the c() function"
  },
  {
    "objectID": "subsetting-and-manipulation.html#matrices",
    "href": "subsetting-and-manipulation.html#matrices",
    "title": "Subsetting and Manipulation",
    "section": "2.3 Matrices",
    "text": "2.3 Matrices\nMatrix objects have 2 dimensions denoted as my_matrix[1:i, 1:j], where i is the number of rows and j is the number of columns.\nTry this in your example script:\n\nmy_matrix <- matrix(data = c(2,3,4,5,6,6,6,6),\n                    nrow = 2, byrow = T)\n\nmy_matrix # notice how the arguments arranged the data\n\n# Flash challenge: make a matrix with the same data vector above to look like...\n#      [,1] [,2]\n# [1,]    2    6\n# [2,]    3    6\n# [3,]    4    6\n# [4,]    5    6\n\n# \"Slicing\" out a row or column\nmy_matrix[1,  ] # Slice out row 1\nmy_matric[ , 3] # Slice out column 3\n\n# Matrix columns and rows often have names\nnames(my_matrix) # No names yet\n\nnrow(my_matrix) # Returns number of rows (useful for large matrices)\nrownames(my_matrix) # No row names; 2 rows, need two names\n\nrownames(my_matrix) <- c(\"dogs\", \"cats\")\nmy_matrix # Now the rows have names!\nrownames(my_matrix) # Get them this way too!\n\n# Flash challenge: Name the columns of my_matrix \"a\", \"b\", \"c\", \"d\" with colnames()\n\nmy_matrix\n\n# Should look like this:\n#      a b c d\n# dogs 2 3 4 5\n# cats 6 6 6 6\n\n# You can also slice out matrix portions by name\nmy_matrix[\"dogs\", c(\"b\", \"d\")]\n\n# Finally, functions act on values, not the index\nmean(my_matrix[\"dogs\", c(\"b\", \"d\")])"
  },
  {
    "objectID": "subsetting-and-manipulation.html#arrays",
    "href": "subsetting-and-manipulation.html#arrays",
    "title": "Subsetting and Manipulation",
    "section": "2.4 Arrays",
    "text": "2.4 Arrays\nArrays are data objects with more than 2 dimensions (well, technically a matrix with 2 dimensions is also an array, but let’s ignore that for now). Array dimensions are denoted as my_array[1:i, 1:j, 1:k], where i is the number of rows and j the columns and k the “depth” of i * j.\nTry this in your example script:\n\n# help(runif)\n# help(round)\n# Try it to see what it does... \nmy_vec <- round(runif(n = 27, min = 0, max = 100), 0)\nmy_vec # See what we did there?\n\nlength(my_vec) # Just checking\n\nmy_array <- array(data = my_vec,\n                  dim = c(3, 3, 3))\nmy_array\n\n# Flash challenge: \n# Specify and print the 1st and 3rd  slice of the k dimension of my_array\n# Assuming my_array has dimensions i, j, k like my_array[i,j,k]"
  },
  {
    "objectID": "subsetting-and-manipulation.html#orchardsprays-data",
    "href": "subsetting-and-manipulation.html#orchardsprays-data",
    "title": "Subsetting and Manipulation",
    "section": "4.1 OrchardSprays data",
    "text": "4.1 OrchardSprays data\nThis is a classic dataset based on an experiment looking at how a chemical additive could be used to deter honeybees from being attracted to crops and subsequently killed by pesticides.\nThe experiment involved having a treatment consisting of adding a “lime sulfur emulsion” (honeybee deterrent) in increasing concentrations to a sucrose solution. The treatment variable had 8 levels including a control (no deterrent) and 7 other levels with increasing concentration of the deterrent. The treatment levels were named A (the highest amount of deterrent), B (second highest deterrent) through to G (lowest deterrent) and H (control - no deterrent) The decrease variable was a measure of the quantity of sucrose solution that was taken by honeybees (the prediction here is that higher concentrations of the deterrent should result in a lower decrease in the sucrose solution).\nThe experiment involved a Latin Square design, with the order of the 8 treatments arranged randomly in an array of 8 columns (the purpose of this design is to randomize any effect of the treatment ORDER or POSITION on the response variable). This resulted in an 8 row by 8 column experiment. The response was measured after placing 100 honeybees into an experimental chamber with the 64 containers of sucrose solution.\n\n## OrchardSprays ####\n## Understand the data - an important step\n\n# Try this\n# Load the OrchardSpray data using the data() function\ndata(OrchardSprays) # Should see OrchardSprays <promise> in the Global Env.\n\n# Look at the data head()\nhead(OrchardSprays) # First 6 rows\n\n# Look at variable types with str()\nhelp(str) # Good function to see info about data object\nstr(OrchardSprays)\n\n# First let's just look at the data\n# Don't worry too much about the code for these graphs if you have not encountered it before\nboxplot(decrease ~ treatment, data = OrchardSprays, \n        main = \"The pattern fits the prediction\",\n        ylab = \"Amount of sucrose consumed\",\n        xlab = \"Lime sulpher treatment amount in decreasing order (H = control)\")\n\n# This is the experimental design\n# Latin Square is kind of like Sudoku\n# No treatment can be in row or column more than once\nplot(x = OrchardSprays$colpos,  # NB use of $ syntax to access data\n     y = OrchardSprays$rowpos, \n     pch = as.character(OrchardSprays$treatment),\n     xlim = c(0,9), ylim = c(0,9),\n     main = \"The Latin Square design of treatments\",\n     xlab = \"\\\"Column\\\" position\",\n     ylab = \"\\\"Row\\\" position\")"
  },
  {
    "objectID": "subsetting-and-manipulation.html#practice-selecting-parts-of-a-data-frame",
    "href": "subsetting-and-manipulation.html#practice-selecting-parts-of-a-data-frame",
    "title": "Subsetting and Manipulation",
    "section": "4.2 Practice Selecting Parts of a Data Frame",
    "text": "4.2 Practice Selecting Parts of a Data Frame\nSelecting particular parts of a data frame based on the values of one variable is a common and extremely useful task.\n\n## Practice selecting parts a data frame ####\n\n# Select the rows of the dataset for treatment \"D\"\n\n# (Pseudocode steps to solve) \n# Break it down to make small steps easy to read\n\n# 01 Boolean phrase to identify rows where treatment value is \"D\"\n# 02 which() to obtain index of TRUE in boolean vector\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\n\n# 01 Boolean phrase\nOrchardSprays$treatment # Just print variable to compare visually to boolean\nOrchardSprays$treatment == \"D\" # logical vector - TRUE in \"D\" positions\n\n# 02 which()\nwhich(OrchardSprays$treatment == \"D\") # Index of TRUE values\nmy_selection <- which(OrchardSprays$treatment == \"D\") # Place index in a variable\nmy_selection # Just checking\n\n# 03 Exploit [ , ] syntax with data frame object to slice out rows\nOrchardSprays[my_selection, ]\n\n# Flash challenge: Select and print all rows at \"colpos\" values of 2"
  },
  {
    "objectID": "subsetting-and-manipulation.html#selection-based-on-multiple-variable-values",
    "href": "subsetting-and-manipulation.html#selection-based-on-multiple-variable-values",
    "title": "Subsetting and Manipulation",
    "section": "4.3 Selection Based on Multiple Variable Values",
    "text": "4.3 Selection Based on Multiple Variable Values\nUsing the basic building blocks of boolean selection, more complex rules for selecting data can be made.\n\n## Compound boolean for selection ####\n\n# Select all rows of the data frame where \n# rowpos equals 4 OR 6 AND treatment equals \"A\" OR \"H\"\n# What we expect is exactly 2 values (A or H) for each powpos (4 or 6)\n\n# rowpos 4 and 6\nOrchardSprays$rowpos == 4 # The 4s\nOrchardSprays$rowpos == 6 # The 6s\n\nOrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6 # All together\n\n# now with which()\nwhich(OrchardSprays$rowpos == 4) # The 4s\nwhich(OrchardSprays$rowpos == 6) # The 6s\n\nwhich(OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) # All together\n\n# treatment A and H\nwhich(OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") # All together\n\n# Now we need the intersection of value that are in both our which() vectors\n\nwhich((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  # It works\n        (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n  \n# NB this is a long way of spelling out our selection, \n# but trying to be very explicit with what is going on\n\nmy_selec2 <- which((OrchardSprays$rowpos == 4 | OrchardSprays$rowpos == 6) &  \n                     (OrchardSprays$treatment == \"A\" | OrchardSprays$treatment == \"H\") ) \n\nOrchardSprays[my_selec2, ] # Double check it works and is similar to expectation...\n\n# Flash challenge: Calculate the mean of decrease for treatment \"A\" \n# and the mean of decrease for treatment \"H\""
  },
  {
    "objectID": "syntax-basics.html",
    "href": "syntax-basics.html",
    "title": "Syntax - basics",
    "section": "",
    "text": "For this section of tutorials we will work on the ‘syntax’, or programming language, of R. You are not expected to be bilingual in this language but just become familiar with the basics, and know where to look for help if needed.\nOn this page, you will find:\n\nExample script, comments, help, pseudocode\nR as a ‘calculator’\nTRUE or FALSE\nRegarding “base R” and the Tidyverse\nPractice exercises"
  },
  {
    "objectID": "syntax-basics.html#help-function",
    "href": "syntax-basics.html#help-function",
    "title": "Syntax - basics",
    "section": "3.1 Help function",
    "text": "3.1 Help function\nOne of the great things about using R is the community and the spirit of helping others. However, there are so many websites, books, blogs and other resources that it can be overwhelming. Best practice is to learn to use the R Help system first, then seek help elsewhere.\nThe basic way to access the built-in help in R, is to use the help() function, with the name of tool you need help using inside the brackets. You may also use ? as a shortcut for help, used before the function.\nFor example, to calculate the mean of some numbers, we would use the function mean(), and to display the help for the mean() function we would run either help(mean) or ?mean. Let’s stick to one for now. Run the following code in your own script:\n\n# Display help page for the function mean\nhelp(mean)\n\nLet’s have a look at the information that is here because the help pages are essential to understand and every help page on every subject is organised in exactly the same way (and we will practice a lot using them).\n\n\n\nPage displayed after running help() function, with annotated section described below\n\n\n1 Function name {Package name} This field let’s you know what “R package” the function belongs to. We can ignore this for now, but it can be very useful.\n2 Short description This tells you in a few words what the function does.\n3 (longer) description This gives a longer description of what the function does\n4 Usage This usually gives an example of the function in use and lists the “arguments” that you are required to supply to the function for it to work on. Of course, you need to know about the arguments...\n5 Argument definitions This field tells you what the argument are and do!\nUsing the Usage and Argument fields, we can figure out how to make the function do the work we want.\n\n# Under Usage:\n# mean(x, ...)\n\n# The \"x\" is an argument that is required\n# The \"...\" means there are other optional arguments\n\n# Under Arguments:\n\n# x \n# An R object... for numeric/logical vectors ...\n\n# try this code in your own script\nmy_length <- c(101, 122, 97) # 3 numerical measures\nmean(x = my_length)"
  },
  {
    "objectID": "syntax-basics.html#organising-code",
    "href": "syntax-basics.html#organising-code",
    "title": "Syntax - basics",
    "section": "3.2 Organising code",
    "text": "3.2 Organising code\nBreaking up a big task into a series of smaller tasks is commonly referred to as PSEUDOCODE or mini code chunks. An example of a task might be ANALYSE YOUR DATA (in shouty capitals because it is a big task). To accomplish this task, we might have to walk through a series of steps, e.g.,\n-Read data into R\n-Test assumption for statistical testing\n-Graph the data\n-Perform statistical test\n-Organize outputs to communicate in report\nIt is often a good idea to break down a task into pseudocode both to organise and document the methods in a logical way, but also to conceptually simplify a problem that is difficult to solve. Practically, the items in a typical table of contents in an R script might be similar to pseudocode.\nNB: This technique can extend well to any problem, not just R code and programming!"
  },
  {
    "objectID": "syntax-basics.html#basic-arithmetic",
    "href": "syntax-basics.html#basic-arithmetic",
    "title": "Syntax - basics",
    "section": "4.1 Basic Arithmetic",
    "text": "4.1 Basic Arithmetic\nBasic manipulation of numbers in R is very easy to do and is so intuitive that you can probably guess what to do and how to do it. There are just a few specifics that we will practice. This list is not exhaustive; the goal is to get enough to begin practicing.\nTry the following in your practice script:\n\n# Add with \"+\"\n2 + 5\n\n# Subtract with \"-\"\n10 - 15\n\n# Multiply with \"*\"\n6 * 4.2\n\n# Divide by \"/\"\n\n10 / 4\n\n# raise to the power of x\n2^3 \n9^(1/2) # same as sqrt()!\n\n# There are a few others, but these are the basics\n\nIt should deliver the following:\n\n# Add with \"+\"\n2 + 5\n\n[1] 7\n\n# Subtract with \"-\"\n10 - 15\n\n[1] -5\n\n# Multiply with \"*\"\n6 * 4.2\n\n[1] 25.2\n\n# Divide by \"/\"\n\n10 / 4\n\n[1] 2.5\n\n# raise to the power of x\n2^3 \n\n[1] 8\n\n9^(1/2) # same as sqrt()!\n\n[1] 3\n\n# There are a few others, but these are the basics"
  },
  {
    "objectID": "syntax-basics.html#order-of-operation",
    "href": "syntax-basics.html#order-of-operation",
    "title": "Syntax - basics",
    "section": "4.2 Order of Operation",
    "text": "4.2 Order of Operation\nRemember BODMAS?\nB-Brackets, O-Orders (powers/indices or roots), D-Division, M-Multiplication, A-Addition, S-Subtraction\nThis principle, or the ‘order of operation’, refers to the order in which mathematical calculations are carried out. A phrase like 2 + 2 is simple, but we need to consider order for more complicated phrases like 2 + 2 * 8 - 6. In general multiplication and division are carried out before addition and subtraction unless specific order is coded. Run this in your example script:\n\n# No order control\n4 + 2 * 3\n\n# Order control - same\n4 + (2 * 3)\n\n# Order control - different...\n(4 + 2) * 3\n\nPlease note: In most cases, the use of spaces does not matter in the R language. Which one of the following ways of writing math operation might be easier to document and read?\n\n# Try this\n\n6+10                                  # no spaces\n7     -5                              # uneven spaces\n1.6             /                2.3  # large spaces\n16 * 3                                # exactly 1 space\n\n# exactly 1 space is probably easiest to read...\n\nYour output should look like this:\n\n6+10                                  # no spaces\n\n[1] 16\n\n7     -5                              # uneven spaces\n\n[1] 2\n\n1.6             /                2.3  # large spaces\n\n[1] 0.6956522\n\n16 * 3                                # exactly 1 space\n\n[1] 48"
  },
  {
    "objectID": "syntax-basics.html#selecting-data",
    "href": "syntax-basics.html#selecting-data",
    "title": "Syntax - basics",
    "section": "5.1 Selecting Data",
    "text": "5.1 Selecting Data\nThese Boolean expressions are often used to select groups of data, for example asking whether values in a column of variables are greater than some threshold. This may be used as an alternative to creating different versions of a particular dataset.\nTry this in your example script:\n\n# Put some data into a variable and then print the variable\n# Note \"<-\" is the ASSIGNMENT syntax in R, which puts the value on the left \"into\" x\n\nx <- c(21, 3, 5, 6, 22)\nx\n\nx > 20\n\n# the square brackets act as the index for the data vector, i.e., everything that applies to the written expression\nx[x > 20]\n\nTo achieve the opposite of this, i.e. NOT selecting data that fits your expression, you can use the ! operator. In some instances this may be a simpler option.\nTry this in your example script:\n\nTRUE # plain true\n\n!FALSE # not false is true!\n\n6 < 5 #definitely false\n\n!(6 < 5) #not false...\n\n!(c(23, 44, 16, 51, 12) > 50)"
  }
]